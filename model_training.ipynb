{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load a file to a variable\n",
    "with open('C:/ntpu_project/dicom_label2.pickle', 'rb') as file:\n",
    "    pickle_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label0 = pickle_dict.get('label0')\n",
    "label1 = pickle_dict.get('label1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len label0 = 4856\n",
      "len label1 = 1835\n"
     ]
    }
   ],
   "source": [
    "print(f'len label0 = {len(label0)}')\n",
    "print(f'len label1 = {len(label1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "import traceback\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepair data for CNN\n",
    "x = list()\n",
    "y = list()\n",
    "all_imgs = list()\n",
    "\n",
    "for dicom in label0:\n",
    "    all_imgs.append(np.expand_dims(dicom.standardlize(), axis=2))\n",
    "    y.append([1])\n",
    "\n",
    "for dicom in label1:\n",
    "    all_imgs.append(np.expand_dims(dicom.standardlize(), axis=2))\n",
    "    y.append([0])\n",
    "    \n",
    "y = np.array(y)\n",
    "x = np.stack(all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(6691, 128, 128, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(6691, 1)\n",
      "4856\n"
     ]
    }
   ],
   "source": [
    "print(type(x))\n",
    "print(np.shape(x))\n",
    "print(type(y))\n",
    "print(np.shape(y))\n",
    "print(np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1835\n"
     ]
    }
   ],
   "source": [
    "print(len(y)-np.sum(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1835, 128, 128, 1)\n",
      "(1835, 1)\n",
      "(1835, 128, 128, 1)\n",
      "(1835, 1)\n",
      "(3670, 128, 128, 1)\n",
      "(3670, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index_list = random.sample(range(len(x)),1835)  # non-nodule 與 nodule 的樣本數需取一樣\n",
    "\n",
    "x_control = x[index_list]\n",
    "y_control = y[index_list]\n",
    "\n",
    "print(np.shape(x_control))\n",
    "print(np.shape(y_control))\n",
    "\n",
    "tempx = list()\n",
    "tempy = list()\n",
    "for i, result in enumerate(y):\n",
    "    if result[0] == 0:\n",
    "        tempx.append(x[i])\n",
    "        tempy.append(result)\n",
    "\n",
    "print(np.shape(tempx))\n",
    "print(np.shape(tempy))\n",
    "        \n",
    "x_control = np.append(x_control, np.array(tempx), axis=0)\n",
    "y_control = np.append(y_control, np.array(tempy), axis=0)\n",
    "\n",
    "print(np.shape(x_control))\n",
    "print(np.shape(y_control))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_control, y_control, test_size=0.2, random_state=2100, shuffle=True)\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2936, 128, 128, 1)\n",
      "(2936, 1)\n",
      "(2936, 2)\n",
      "1083\n",
      "(734, 128, 128, 1)\n",
      "(734, 1)\n",
      "(734, 2)\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(y_train_onehot))\n",
    "print(np.sum(y_train))\n",
    "\n",
    "print(np.shape(x_test))\n",
    "print(np.shape(y_test))\n",
    "print(np.shape(y_test_onehot))\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.keras import CnnModel, show_train_history\n",
    "from utils.focal_loss import multi_category_focal_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model = CnnModel(x_train, y_train_onehot, x_test, y_test_onehot)\n",
    "cnn_model = CnnModel(x_train, y_train_onehot, x_test, y_test_onehot, [multi_category_focal_loss2()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepairing model config\n",
    "config_list = list()\n",
    "config_list.append({\n",
    "        'conv2D_config_list': [\n",
    "            {'filters': 128, 'kernel_size': (3, 3), 'input_shape': (128, 128, 1), 'strides': 2},\n",
    "            {'filters': 128, 'kernel_size': (3, 3), 'input_shape': (128, 128, 1), 'strides': 2},\n",
    "        ],\n",
    "        'dense_config_list': [\n",
    "            {'unit': 64},\n",
    "            {'unit': 64},\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===processing model 1===\n",
      "Train on 2348 samples, validate on 588 samples\n",
      "Epoch 1/10\n",
      "2348/2348 [==============================] - 1s 517us/sample - loss: 0.0839 - accuracy: 0.6107 - val_loss: 0.0825 - val_accuracy: 0.6480\n",
      "Epoch 2/10\n",
      "2348/2348 [==============================] - 1s 265us/sample - loss: 0.0821 - accuracy: 0.6269 - val_loss: 0.0809 - val_accuracy: 0.6480\n",
      "Epoch 3/10\n",
      "2348/2348 [==============================] - 1s 263us/sample - loss: 0.0809 - accuracy: 0.6273 - val_loss: 0.0808 - val_accuracy: 0.6480\n",
      "Epoch 4/10\n",
      "2348/2348 [==============================] - 1s 260us/sample - loss: 0.0797 - accuracy: 0.6380 - val_loss: 0.0793 - val_accuracy: 0.6684\n",
      "Epoch 5/10\n",
      "2348/2348 [==============================] - 1s 272us/sample - loss: 0.0768 - accuracy: 0.6580 - val_loss: 0.0793 - val_accuracy: 0.6633\n",
      "Epoch 6/10\n",
      "2348/2348 [==============================] - 1s 264us/sample - loss: 0.0721 - accuracy: 0.7070 - val_loss: 0.0819 - val_accuracy: 0.6599\n",
      "Epoch 7/10\n",
      "2348/2348 [==============================] - 1s 264us/sample - loss: 0.0684 - accuracy: 0.7317 - val_loss: 0.0794 - val_accuracy: 0.6565\n",
      "Epoch 8/10\n",
      "2348/2348 [==============================] - 1s 263us/sample - loss: 0.0622 - accuracy: 0.7781 - val_loss: 0.0847 - val_accuracy: 0.5986\n",
      "Epoch 9/10\n",
      "2348/2348 [==============================] - 1s 264us/sample - loss: 0.0549 - accuracy: 0.8088 - val_loss: 0.0872 - val_accuracy: 0.6820\n",
      "Epoch 10/10\n",
      "2348/2348 [==============================] - 1s 264us/sample - loss: 0.0463 - accuracy: 0.8437 - val_loss: 0.0925 - val_accuracy: 0.6752\n",
      "Train accuracy: 0.8436968\n",
      "Train loss: 0.0463349784468975\n",
      "Test accuracy: 0.6852861\n",
      "Test loss: 0.09119576138599042\n",
      "Sensitivity: 0.8041237113402062\n",
      "Specificity: 0.4538152610441767\n",
      "===file output to model_result-model_result_20200624024323.txt.txt===\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "train_history_list = list()\n",
    "\n",
    "for i, config in enumerate(config_list):\n",
    "    print(f'===processing model {(i+1)}===')\n",
    "    train_history_list.append(cnn_model.run(i, 100, 10, config['conv2D_config_list'], config['dense_config_list'], 1))\n",
    "#     print(f'=============111=================')\n",
    "#     train_history_list.append(cnn_model.run(i, 200, 20, config['conv2D_config_list'], config['dense_config_list']))\n",
    "#     print(f'==============================')\n",
    "#     train_history_list.append(cnn_model.run(i, 200, 30, config['conv2D_config_list'], config['dense_config_list']))\n",
    "#     print(f'==============================')\n",
    "#     train_history_list.append(cnn_model.run(i, 200, 40, config['conv2D_config_list'], config['dense_config_list']))\n",
    "    print(f'==============================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for train_history in train_history_list:\n",
    "    show_train_history(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "conv2D_config_list = [\n",
    "    {'filters': 16, 'kernel_size': (2, 2), 'input_shape': (128, 128, 1), 'strides': (2, 2)},\n",
    "    {'filters': 32, 'kernel_size': (2, 2), 'input_shape': (128, 128, 1), 'strides': (2, 2)}\n",
    "]\n",
    "dense_config_list = [\n",
    "    {'unit': 128},\n",
    "    {'unit': 128}\n",
    "]\n",
    "train_history = cnn_model.run(index=index, batch_size=100, epochs=10, conv2D_config_list=conv2D_config_list, dense_config_list=dense_config_list)\n",
    "train_history_list.append(train_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "# import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.75, allow_growth=True)\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/51298992/binary-classifier-keras-callback-for-sensitivity-and-specificity\n",
    "\n",
    "def output_sensitivity_specificity(x_test, y_test):\n",
    "    y_test = np.argmax(y_test, axis=-1)\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    c = confusion_matrix(y_test, predictions)\n",
    "    print('Confusion matrix:\\n', c)\n",
    "    print(f'sensitivity = {c[0, 0] / (c[0, 1] + c[0, 0])}')\n",
    "    print(f'specificity = {c[1, 1] / (c[1, 1] + c[1, 0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "input_shape = np.shape(x_train[0])\n",
    "print(input_shape)\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = SparseCategoricalFocalLoss(gamma=2)\n",
    "# y_true = [0, 1, 2]\n",
    "# y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.2, 0.2, 0.6]]\n",
    "# loss_func(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 128, 128, 32)      160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 4,198,978\n",
      "Trainable params: 4,198,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 建立簡單的線性執行的模型\n",
    "model = Sequential()\n",
    "# 建立卷積層，filter=16,即 output space 的深度, Kernal Size: 5x5, activation function 採用 relu\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(2, 2),\n",
    "                 padding=\"same\",\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "# 建立池化層，池化大小=2x2，取最大值\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(filters=32, \n",
    "                 kernel_size=(2, 2),\n",
    "                 padding=\"same\",\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.25\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flatten層把多維的輸入一維化，常用在從卷積層到全連接層的過渡。\n",
    "model.add(Flatten())\n",
    "\n",
    "# 全連接層: 128個output\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout層隨機斷開輸入神經元，用於防止過度擬合，斷開比例:0.25\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# 使用 softmax activation function，將結果分類\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# 編譯: 選擇損失函數、優化方法及成效衡量方式\n",
    "# model.compile(loss=\"categorical_crossentropy\",\n",
    "#               optimizer=\"adam\",\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=[SigmoidFocalCrossEntropy()], metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "\n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2348 samples, validate on 588 samples\n",
      "Epoch 1/10\n",
      "2348/2348 [==============================] - 1s 292us/sample - loss: 0.0098 - accuracy: 0.9672 - val_loss: 0.2436 - val_accuracy: 0.6990\n",
      "Epoch 2/10\n",
      "2348/2348 [==============================] - 1s 285us/sample - loss: 0.0097 - accuracy: 0.9664 - val_loss: 0.2228 - val_accuracy: 0.6939\n",
      "Epoch 3/10\n",
      "2348/2348 [==============================] - 1s 290us/sample - loss: 0.0091 - accuracy: 0.9702 - val_loss: 0.2165 - val_accuracy: 0.7007\n",
      "Epoch 4/10\n",
      "2348/2348 [==============================] - 1s 292us/sample - loss: 0.0111 - accuracy: 0.9634 - val_loss: 0.1992 - val_accuracy: 0.7007\n",
      "Epoch 5/10\n",
      "2348/2348 [==============================] - 1s 290us/sample - loss: 0.0105 - accuracy: 0.9681 - val_loss: 0.2150 - val_accuracy: 0.7007\n",
      "Epoch 6/10\n",
      "2348/2348 [==============================] - 1s 285us/sample - loss: 0.0096 - accuracy: 0.9629 - val_loss: 0.2359 - val_accuracy: 0.7143\n",
      "Epoch 7/10\n",
      "2348/2348 [==============================] - 1s 292us/sample - loss: 0.0089 - accuracy: 0.9698 - val_loss: 0.2275 - val_accuracy: 0.7075\n",
      "Epoch 8/10\n",
      "2348/2348 [==============================] - 1s 291us/sample - loss: 0.0097 - accuracy: 0.9710 - val_loss: 0.2210 - val_accuracy: 0.7092\n",
      "Epoch 9/10\n",
      "2348/2348 [==============================] - 1s 290us/sample - loss: 0.0086 - accuracy: 0.9736 - val_loss: 0.2781 - val_accuracy: 0.7109\n",
      "Epoch 10/10\n",
      "2348/2348 [==============================] - 1s 284us/sample - loss: 0.0102 - accuracy: 0.9668 - val_loss: 0.1980 - val_accuracy: 0.7075\n",
      "Test loss: 0.21464255261323759\n",
      "Test accuracy: 0.7070845\n",
      "Confusion matrix:\n",
      " [[384  81]\n",
      " [134 135]]\n",
      "sensitivity = 0.8258064516129032\n",
      "specificity = 0.5018587360594795\n"
     ]
    }
   ],
   "source": [
    "# 進行訓練, 訓練過程會存在 train_history 變數中\n",
    "\n",
    "train_history = model.fit(x_train, \n",
    "                          y_train_onehot,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=epochs,\n",
    "                          verbose=1,\n",
    "                          validation_split=0.2)\n",
    "\n",
    "# 顯示損失函數、訓練成果(分數)\n",
    "score = model.evaluate(x_test, y_test_onehot, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "output_sensitivity_specificity(x_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(734, 128, 128, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(734, 2)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_test))\n",
    "print(np.shape(x_test))\n",
    "print(type(y_test_onehot))\n",
    "print(np.shape(y_test_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1318, 128, 128, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "(1318, 2)\n",
      "nodule count = 674\n"
     ]
    }
   ],
   "source": [
    "print(type(x_test_same_size))\n",
    "print(np.shape(x_test_same_size))\n",
    "print(type(y_test_onehot_same_size))\n",
    "print(np.shape(y_test_onehot_same_size))\n",
    "\n",
    "print(f'nodule count = {np.sum(y_test_same_size)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9626927503493469\n",
      "Test accuracy: 0.60622156\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test_same_size, y_test_onehot_same_size, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "par1={\n",
    "    'Validation': 0.2, \n",
    "    'epoch': 40, \n",
    "    'Batch': 10\n",
    "}\n",
    "par = {\n",
    "    'filter1':64,\n",
    "    'filter2':128,\n",
    "    'strides':1,\n",
    "    'kernel': 2,\n",
    "    'dropout1':0.5,\n",
    "    'dropout2':0.5,\n",
    "    'Neural':256\n",
    "}\n",
    "\n",
    "def DL_fun1(par,par1,x_train,y_train,x_test,y_test):\n",
    "    result = None\n",
    "    nf=par['filter1']\n",
    "    nf1=par['filter2']\n",
    "    ks=par['kernel']\n",
    "    hs=np.shape(x)[1]\n",
    "    ws=np.shape(x)[2]\n",
    "    dr1=par['dropout1']\n",
    "    dr2=par['dropout2']\n",
    "    nn=par['Neural']\n",
    "    vsr=par1['Validation']\n",
    "    epn=par1['epoch']\n",
    "    bs=par1['Batch']\n",
    "    y_train_onehot = to_categorical(y_train)\n",
    "    y_test_onehot = to_categorical(y_test)\n",
    "    try:\n",
    "        # 只使用 80% 的 GPU 記憶體config = tf.ConfigProto()\n",
    "#         gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5, allow_growth=True)\n",
    "#         sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#         tf.keras.backend.set_session(sess)\n",
    "\n",
    "        model = Sequential([\n",
    "            Conv2D(filters=nf,kernel_size=(ks,ks),strides=(par['strides'],par['strides']),padding='same',input_shape=(hs,ws,1),activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(filters=nf1,kernel_size=(ks,ks),strides=(par['strides'],par['strides']),padding='same',input_shape=(hs,ws,1),activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Dropout(dr1),\n",
    "            Flatten(),\n",
    "            Dense(nn, activation='relu'),\n",
    "            Dropout(dr2),\n",
    "            Dense(3,activation='softmax'),\n",
    "        ])\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        print(model.summary()) \n",
    "        train_history=model.fit(x=x_train,\n",
    "                                y=y_train_onehot,\n",
    "                                validation_split=vsr,\n",
    "                                epochs=epn,\n",
    "                                batch_size=bs,\n",
    "                                verbose=2)\n",
    "        scores=model.evaluate(x=x_test,y=y_test_onehot)\n",
    "        result={'History':train_history, 'Score': scores}\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(traceback.format_exc())\n",
    "    finally:\n",
    "#         sess.close()\n",
    "        pass\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result=DL_fun1(par,par1,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_category_focal_loss2(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    focal loss for multi category of multi label problem\n",
    "    适用于多分类或多标签问题的focal loss\n",
    "    alpha控制真值y_true为1/0时的权重\n",
    "        1的权重为alpha, 0的权重为1-alpha\n",
    "    当你的模型欠拟合，学习存在困难时，可以尝试适用本函数作为loss\n",
    "    当模型过于激进(无论何时总是倾向于预测出1),尝试将alpha调小\n",
    "    当模型过于惰性(无论何时总是倾向于预测出0,或是某一个固定的常数,说明没有学到有效特征)\n",
    "        尝试将alpha调大,鼓励模型进行预测出1。\n",
    "    Usage:\n",
    "     model.compile(loss=[multi_category_focal_loss2(alpha=0.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    epsilon = 1.e-7\n",
    "    gamma = float(gamma)\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "\n",
    "    def multi_category_focal_loss2_fixed(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "    \n",
    "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
    "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
    "        ce = -tf.math.log(y_t)\n",
    "        weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
    "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
    "        loss = tf.reduce_mean(fl)\n",
    "        return loss\n",
    "    return multi_category_focal_loss2_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.losses import LossFunctionWrapper\n",
    "\n",
    "class SigmoidFocalCrossEntropy(LossFunctionWrapper):\n",
    "    \"\"\"Implements the focal loss function.\n",
    "    Focal loss was first introduced in the RetinaNet paper\n",
    "    (https://arxiv.org/pdf/1708.02002.pdf). Focal loss is extremely useful for\n",
    "    classification when you have highly imbalanced classes. It down-weights\n",
    "    well-classified examples and focuses on hard examples. The loss value is\n",
    "    much high for a sample which is misclassified by the classifier as compared\n",
    "    to the loss value corresponding to a well-classified example. One of the\n",
    "    best use-cases of focal loss is its usage in object detection where the\n",
    "    imbalance between the background class and other classes is extremely high.\n",
    "    Usage:\n",
    "    ```python\n",
    "    fl = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "    loss = fl(\n",
    "      [[0.97], [0.91], [0.03]],\n",
    "      [[1.0], [1.0], [0.0]])\n",
    "    print('Loss: ', loss.numpy())  # Loss: [0.00010971,\n",
    "                                            0.0032975,\n",
    "                                            0.00030611]\n",
    "    ```\n",
    "    Usage with tf.keras API:\n",
    "    ```python\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile('sgd', loss=tf.keras.losses.SigmoidFocalCrossEntropy())\n",
    "    ```\n",
    "    Args\n",
    "      alpha: balancing factor, default value is 0.25\n",
    "      gamma: modulating factor, default value is 2.0\n",
    "    Returns:\n",
    "      Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same\n",
    "          shape as `y_true`; otherwise, it is scalar.\n",
    "    Raises:\n",
    "        ValueError: If the shape of `sample_weight` is invalid or value of\n",
    "          `gamma` is less than zero\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        from_logits=False,\n",
    "        alpha=0.25,\n",
    "        gamma=2.0,\n",
    "        reduction: str = tf.losses.Reduction.NONE,\n",
    "        name: str = \"sigmoid_focal_crossentropy\",\n",
    "    ):\n",
    "        super().__init__(\n",
    "            sigmoid_focal_crossentropy,\n",
    "            name=name,\n",
    "            reduction=reduction,\n",
    "            from_logits=from_logits,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "        )\n",
    "\n",
    "def sigmoid_focal_crossentropy(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    alpha=0.25,\n",
    "    gamma=2.0,\n",
    "    from_logits: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        y_true: true targets tensor.\n",
    "        y_pred: predictions tensor.\n",
    "        alpha: balancing factor.\n",
    "        gamma: modulating factor.\n",
    "    Returns:\n",
    "        Weighted loss float `Tensor`. If `reduction` is `NONE`,this has the\n",
    "        same shape as `y_true`; otherwise, it is scalar.\n",
    "    \"\"\"\n",
    "    if gamma and gamma < 0:\n",
    "        raise ValueError(\"Value of gamma should be greater than or equal to zero\")\n",
    "\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=y_pred.dtype)\n",
    "\n",
    "    # Get the cross_entropy for each entry\n",
    "    ce = K.binary_crossentropy(y_true, y_pred, from_logits=from_logits)\n",
    "\n",
    "    # If logits are provided then convert the predictions into probabilities\n",
    "    if from_logits:\n",
    "        pred_prob = tf.sigmoid(y_pred)\n",
    "    else:\n",
    "        pred_prob = y_pred\n",
    "\n",
    "    p_t = (y_true * pred_prob) + ((1 - y_true) * (1 - pred_prob))\n",
    "    alpha_factor = 1.0\n",
    "    modulating_factor = 1.0\n",
    "\n",
    "    if alpha:\n",
    "        alpha = tf.convert_to_tensor(alpha, dtype=K.floatx())\n",
    "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "\n",
    "    if gamma:\n",
    "        gamma = tf.convert_to_tensor(gamma, dtype=K.floatx())\n",
    "        modulating_factor = tf.pow((1.0 - p_t), gamma)\n",
    "\n",
    "    # compute the final loss and return\n",
    "    return tf.reduce_sum(alpha_factor * modulating_factor * ce, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  [6.8532740e-06 1.9097868e-04 2.0559823e-05]\n"
     ]
    }
   ],
   "source": [
    "fl = SigmoidFocalCrossEntropy()\n",
    "loss = fl(\n",
    "  y_true = [[1.0], [1.0], [0.0]],\n",
    "  y_pred = [[0.97], [0.91], [0.03]])\n",
    "print('Loss: ', loss.numpy())  # Loss: [6.8532745e-06,\n",
    "#                                         1.9097870e-04,\n",
    "#                                         2.0559824e-05]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
